# Movie Review Sentiment Analysis

## Overview
This project builds a classifier to predict positive or negative sentiment from IMDb movie reviews using classical ML and deep learning (LSTM) approaches.


## Inspiration
This project was inspired by several resources on sentiment analysis:

- ğŸ‘‰ **RoadMap:** [Machine Learning Roadmap](https://roadmap.sh/machine-learning)  
- ğŸ¯ **Kaggle Competition:** [Sentiment Analysis on Movie Reviews](https://www.kaggle.com/competitions/sentiment-analysis-on-movie-reviews/overview)  
- ğŸ“š **GeeksforGeeks Tutorial:** [Sentiment Analysis on IMDb Movie Reviews](https://www.geeksforgeeks.org/nlp/sentiment-analysis-on-imdb-movie-reviews/)  
- ğŸ”¥ **PyTorch Quickstart Tutorial:** [Beginner Basics](https://docs.pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html)  
- ğŸ’¡ **GitHub Repo:** [PyTorch Sentiment Analysis by Ben Trevett](https://github.com/bentrevett/pytorch-sentiment-analysis) â€” helped shape the LSTM implementation and overall project structure


## Project Folders Documentation

- [src folder README](movie-sentiment-analysis/README-ML.md) â€“ contains scripts for training and evaluating models



## âœ¨ Features
- **Text preprocessing**: Tokenization, stopword removal, lemmatization
- **ML models**: Logistic Regression, Naive Bayes, SVM
- **Deep learning**: PyTorch LSTM with embeddings
- **Evaluation**: Accuracy, precision, recall, F1-score, confusion matrices, ROC curves

## ğŸ› ï¸ Tech Stack
Python, PyTorch, TorchText, scikit-learn, NLTK, SpaCy, Matplotlib, Seaborn, Jupyter Notebook

---

## ğŸš€ Quick Start

### 1. Setup Environment

```bash
# Clone/create project directory
mkdir movie-sentiment
cd movie-sentiment

# Create directories
mkdir data src models results

# Create virtual environment (recommended)
python -m venv venv
venv\Scripts\activate  # Windows
# source venv/bin/activate  # Linux/Mac

# Install dependencies
pip install -r requirements.txt
```

### 2. Get the Dataset

1. Download **IMDB Dataset of 50K Movie Reviews** from:
   - Kaggle: https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews
   
2. Place `IMDB Dataset.csv` in the `data/` folder:
   ```
   movie-sentiment/data/IMDB Dataset.csv
   ```

### 3. Add Project Files

Place these files in your project:
- `config.py` â†’ Root directory
- `src/preprocess.py`
- `src/train_ml.py`
- `src/train_lstm.py`
- `src/evaluate.py`

### 4. Run the Pipeline

Execute these commands in order:

```bash
# Step 1: Preprocess data (5-10 minutes)
python src/preprocess.py

# Step 2: Train ML models (3-5 minutes)
python src/train_ml.py

# Step 3: Train LSTM model (30-60 min CPU, 5-10 min GPU)
python src/train_lstm.py

# Step 4: Evaluate and visualize (2 minutes)
python src/evaluate.py
```

### 5. View Results

Check the `results/` folder for:
- Confusion matrices for each model
- ROC curves comparison
- Model performance comparison chart
- Detailed metrics table (CSV)

---

## ğŸ“Š Expected Results

| Model               | Accuracy | F1-Score |
|---------------------|----------|----------|
| Logistic Regression | ~88%     | 0.88     |
| Naive Bayes         | ~86%     | 0.86     |
| SVM                 | ~89%     | 0.89     |
| **LSTM**            | **~90%** | **0.90** |

---

## ğŸ“ Project Structure

```
movie-sentiment/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ IMDB Dataset.csv           # Raw dataset (YOU ADD THIS)
â”‚   â””â”€â”€ preprocessed_data.pkl      # Auto-generated
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocess.py              # Data cleaning & splitting
â”‚   â”œâ”€â”€ train_ml.py                # Train ML models
â”‚   â”œâ”€â”€ train_lstm.py              # Train LSTM model
â”‚   â””â”€â”€ evaluate.py                # Evaluation & visualization
â”‚
â”œâ”€â”€ models/                         # Saved models (auto-created)
â”œâ”€â”€ results/                        # Plots & metrics (auto-created)
â”œâ”€â”€ config.py                       # Configuration
â”œâ”€â”€ requirements.txt                # Dependencies
â””â”€â”€ README.md                       # This file
```

---

## ğŸ”§ Configuration

Edit `config.py` to customize:

```python
# Data
DATA_PATH = 'data/IMDB Dataset.csv'
TEST_SIZE = 0.2  # 20% for testing

# ML Models
TFIDF_MAX_FEATURES = 5000

# LSTM
EMBED_DIM = 100
HIDDEN_DIM = 128
N_LAYERS = 2
BATCH_SIZE = 64
N_EPOCHS = 5
LEARNING_RATE = 0.001
```

---

## ğŸ“– Documentation

Each Python file contains detailed documentation:

### `src/preprocess.py`
- Loads and cleans text data
- Removes HTML, special characters
- Tokenizes, removes stopwords, lemmatizes
- Splits into train/test sets (80/20)

### `src/train_ml.py`
- Converts text to TF-IDF features
- Trains Logistic Regression, Naive Bayes, SVM
- Evaluates on test set
- Saves trained models

### `src/train_lstm.py`
- Builds vocabulary from training data
- Encodes text as sequences
- Trains LSTM neural network
- Uses GPU if available
- Saves best model

### `src/evaluate.py`
- Loads all trained models
- Computes accuracy, precision, recall, F1
- Generates confusion matrices
- Creates ROC curves
- Saves comparison visualizations

---

## ğŸ› Troubleshooting

### Dataset Not Found
```
âŒ ERROR: File not found at data/IMDB Dataset.csv
```
**Solution**: Download and place the dataset in the `data/` folder

### Preprocessed Data Missing
```
âŒ ERROR: preprocessed_data.pkl not found!
```
**Solution**: Run `python src/preprocess.py` first

### NLTK Data Missing
```python
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')
```

### Out of Memory (LSTM)
**Solution**: Reduce `BATCH_SIZE` in `config.py` (try 32 or 16)

### Slow Training
- **Check GPU**: `torch.cuda.is_available()` should return `True`
- **Reduce epochs**: Set `N_EPOCHS = 3` in `config.py`
- **Use smaller vocab**: Set `MAX_VOCAB_SIZE = 5000`

---

## ğŸ“ Key Concepts

### ML Models
- **Logistic Regression**: Linear classifier, fast and interpretable
- **Naive Bayes**: Probabilistic model based on word frequencies
- **SVM**: Finds optimal decision boundary between classes

### LSTM (Long Short-Term Memory)
- Neural network that processes sequences
- Maintains "memory" of previous words
- Captures context and word order
- Better than traditional RNN at long sequences

### Metrics
- **Accuracy**: % of correct predictions
- **Precision**: Of predicted positives, % that were correct
- **Recall**: Of actual positives, % that were detected
- **F1-Score**: Harmonic mean of precision and recall

---

## ğŸ“ˆ Next Steps

### Improve the Model
- Use pre-trained embeddings (GloVe, Word2Vec)
- Try bidirectional LSTM
- Implement attention mechanism
- Use Transformer models (BERT, RoBERTa)

### Add Features
- Web interface for predictions
- Real-time sentiment analysis
- Multi-class classification (1-5 stars)
- Aspect-based sentiment analysis

### Deploy
- Create Flask/FastAPI API
- Build Streamlit dashboard
- Deploy to AWS/Heroku/Google Cloud

---

## ğŸ“¦ Requirements

```
numpy
pandas
scikit-learn
torch
torchtext
matplotlib
seaborn
nltk
spacy
jupyter
```

See `requirements.txt` for exact versions.

---

## ğŸ¤ Contributing

This is a learning project. Feel free to:
- Experiment with different architectures
- Try different hyperparameters
- Add new features
- Improve documentation

---

## ğŸ“„ License

This project is for educational purposes.

Dataset: IMDB Movie Reviews (Kaggle)

---

## ğŸ¯ Learning Objectives

After completing this project, you will understand:
- âœ… Text preprocessing techniques
- âœ… Feature extraction (TF-IDF)
- âœ… Classical ML for text classification
- âœ… Neural networks for NLP
- âœ… LSTM architecture and training
- âœ… Model evaluation metrics
- âœ… PyTorch basics
- âœ… Data pipelines

---

## ğŸŒŸ Acknowledgments

- Dataset: Andrew L. Maas et al. (Stanford)
- Frameworks: PyTorch, scikit-learn, NLTK
- Community: Kaggle, Stack Overflow

---





